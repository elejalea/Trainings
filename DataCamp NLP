import re

s = "La fête de Noël est passée très vite!.. Tout le monde est déjà rentré chez soi."
list_s = s.split()                              # преобразовали строку в список

diacritics = r'[^a-zA-Zа-яА-ЯёЁ0-9 .,:;!?«»r''"‎/\{}()<>@&№#^*$  ]'
diacritics_in_s = re.findall(diacritics, s)     # нашли все диакритики в строке
diacritics_set = set(diacritics_in_s)           # найденное преобразуем в множество уникальных диакритик
print(diacritics_set)
diacritic_words = []

for i in diacritics_set:                       # прошлись всеми диакритиками строки
    for j in list_s:                           # по строке, преобразованной в список
        if i in j:
            diacritic_words.append(j)
print(set(diacritic_words))                    # множество уникальных слов с диакритикой (déjà один раз)
____________________________________________________________________
re.findall(pattern, emails, flags=re.IGNORECASE) 

re.findall(r'<p>(.*)</p>', text)

mail = r'.*@\w+\.\w+'

print(re.findall(r'\w+', '443031 Самара'))
Output: ['443031', 'Самара']
____________________________________________________________________
# Write a pattern to match sentence endings: sentence_endings
sentence_endings = r"[.?!]"

# Split my_string on sentence endings and print the result
print(re.split(sentence_endings, my_string))

# Find all capitalized words in my_string and print the result
capitalized_words = r"[A-Z]\w+"
print(re.findall(capitalized_words, my_string))

# Split my_string on spaces and print the result
spaces = r"\s+"
print(re.split(spaces, my_string))

# Find all digits in my_string and print the result
digits = r"\d+"
print(re.findall(digits, my_string))
____________________________________________________________________
TOKENIZATION

from nltk.tokenize import sent_tokenize 
from nltk.tokenize import word_tokenize 

# Split scene_one into sentences: sentences
sentences = sent_tokenize(scene_one)

# Use word_tokenize to tokenize the fourth sentence: tokenized_sent
tokenized_sent = word_tokenize(sentences[3])

# Make a set of unique tokens in the entire scene: unique_tokens
unique_tokens = set(word_tokenize(scene_one))
____________________________________________________________________
from nltk.tokenize import regexp_tokenize, TweetTokenizer 
# Write a pattern that matches both mentions (@) and hashtags
pattern2 = r"([@|#]\w+)"
# Use the pattern on the last tweet in the tweets list
mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)
print(mentions_hashtags)
____________________________________________________________________
from nltk.tokenize import regexp_tokenize
from nltk.tokenize import TweetTokenizer
# Use the TweetTokenizer to tokenize all tweets into one list
tknzr = TweetTokenizer()
all_tokens = [tknzr.tokenize(t) for t in tweets]
print(all_tokens)
____________________________________________________________________
all_words = word_tokenize(german_text)
print(all_words)

capital_words = r"[A-ZÜ]\w+"
print(regexp_tokenize(german_text, capital_words))

emoji = "['\U0001F300-\U0001F5FF'|'\U0001F600-\U0001F64F'|'\U0001F680-\U0001F6FF'|'\u2600-\u26FF\u2700-\u27BF']"
print(regexp_tokenize(german_text, emoji))
____________________________________________________________________
# list comprehension
word_lengths = [len(w) for w in words]

from matplotlib import pyplot as plt
from nltk.tokenize import word_tokenize

# Split the script into lines: lines
lines = holy_grail.split('\n')

# Replace all script lines for speaker
pattern = "[A-Z]{2,}(\s)?(#\d)?([A-Z]{2,})?:"
lines = [re.sub(pattern, '', l) for l in lines]

# Tokenize each line: tokenized_lines
tokenized_lines = [regexp_tokenize(s, r"\w+") for s in lines]

# Make a frequency list of lengths: line_num_words
line_num_words = [len(t_line) for t_line in tokenized_lines]
plt.hist(line_num_words)
plt.show()
____________________________________________________________________
BAG-OF-WORDS

from collections import Counter
tokens = word_tokenize(article)
lower_tokens = [t.lower() for t in tokens]
                  
# Create a Counter with the lowercase tokens
bow_simple = Counter(lower_tokens)
# Print the 10 most common tokens
print(bow_simple.most_common(10))
____________________________________________________________________
# To remove non-alphabetic
tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()]
# To remove stopwords for English
no_stops = [t for t in tokens if t not in stopwords.words('english')]
print(Counter(no_stops).most_common(2))
____________________________________________________________________
from nltk.stem import WordNetLemmatizer

# Retain alphabetic words: alpha_only
alpha_only = [t for t in lower_tokens if t.isalpha()]
# Remove all stop words: no_stops
no_stops = [t for t in alpha_only if t not in english_stops]

# Instantiate the WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()
# Lemmatize all tokens into a new list: lemmatized
lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]

# Create the bag-of-words: bow
bow = Counter(lemmatized)
# Print the 10 most common tokens
print(bow.most_common(10))
____________________________________________________________________
GENSIM
Порождающая модель LDA. Latent Dirichlet allocation / Латентное размещение Дирихле

from gensim.corpora.dictionary import Dictionary
from nltk.tokenize import word_tokenize
tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]
dictionary = Dictionary(tokenized_docs)
dictionary.token2id
OUT: {'!': 11, ',': 17, '.': 7, 'a': 2 ...}
____________________________________________________________________
# Returns a list of lists representing a bag-of-words. Each list is for a doc.
# The key is a token id, the value is a token frequency.
corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]
OUT: [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],
 [(0, 1), (1, 1), (9, 1), (10, 1), (11, 1), (12, 1)], ...]
____________________________________________________________________
from gensim.corpora.dictionary import Dictionary

# Create a Dictionary from the articles: dictionary
dictionary = Dictionary(articles)

# Select the id for "computer": computer_id
computer_id = dictionary.token2id.get("computer")

# Use computer_id with the dictionary to print the word
print(dictionary.get(computer_id))

# Create a MmCorpus: corpus
corpus = [dictionary.doc2bow(article) for article in articles]

# Print the first 10 word ids with their frequency counts from the fifth document
print(corpus[4][:10])
____________________________________________________________________
DEFAULTDICT

from collections import defaultdict
d = defaultdict(object)
print(d['A'])
OUT: <object object at 0x7fc9bed4cb00>

defdict = collections.defaultdict(list)
print(defdict)
defaultdict(<class 'list'>, {})
for i in range(5):
    defdict[i].append(i)
    print(defdict)
defaultdict(<class 'list'>, {0: [0], 1: [1], 2: [2], 3: [3], 4: [4]})

OrderedDict

d = {'banana': 3, 'apple': 4, 'pear': 1, 'orange': 2}
# словарь, отсортированный по ключу
OrderedDict(sorted(d.items(), key=lambda t: t[0]))
OrderedDict([('apple', 4), ('banana', 3), ('orange', 2), ('pear', 1)])
________________________________________________________________
# Save the fifth document
doc = corpus[4]

# Sort the doc for frequency
bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)

# Print the top 5 words of the doc
for word_id, word_count in bow_doc[:5]:
    print(dictionary.get(word_id), word_count)
    
# Create the defaultdict: total_word_count
total_word_count = defaultdict(int)
for word_id, word_count in itertools.chain.from_iterable(corpus):
    total_word_count[word_id] += word_count
    
# Create a sorted list from the defaultdict, using words across the entire corpus.
sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) 

# Print the top 5 words across all documents alongside the count
for word_id, word_count in sorted_word_count[:5]:
    print(dictionary.get(word_id), word_count) 
________________________________________________________________
from gensim.models.tfidfmodel import TfidfModel

# Create a new TfidfModel using the corpus
tfidf = TfidfModel(corpus)

# Calculate the tfidf weights of doc
tfidf_weights = tfidf[doc]

# Print the first five weights
print(tfidf_weights[:5])

# Sort the weights from highest to lowest
sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)

# Print the top 5 weighted words (term_id) from sorted_tfidf_weights with their weighted score (weight)
for term_id, weight in sorted_tfidf_weights[:5]:
    print(dictionary.get(term_id), weight)
________________________________________________________________
Named Entity Recognition

import nltk
sentence = '''In New York, I like to ride the Metro to visit MOMA 
and some restaurants rated well by Ruth Reichl.'''

tokenized_sent = nltk.word_tokenize(sentence)
tagged_sent = nltk.pos_tag(tokenized_sent)
tagged_sent[:3]
OUT: [('In', 'IN'), ('New', 'NNP'), ('York', 'NNP')]
________________________________________________________________
